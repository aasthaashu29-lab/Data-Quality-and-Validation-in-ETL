{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Data Quality and Validation in ETL**\n",
        "\n",
        "Question 1 : Define Data Quality in the context of ETL pipelines. Why is it more than just data cleaning?\n",
        "\n",
        "    -->Data quality in ETL pipelines refers to the degree to which data is accurate, complete, consistent, timely, valid, and reliable throughout extraction, transformation, and loading.\n",
        "\n",
        "It is more than just data cleaning because:\n",
        "\t•\tData cleaning fixes errors (nulls, duplicates, formats)\n",
        "\t•\tData quality ensures business correctness, rule validation, referential integrity, and trustworthiness\n",
        "\t•\tHigh-quality data supports correct analytics and decision-making, not just tidy datasets\n",
        "\n",
        "\n",
        "Question 2 : Explain why poor data quality leads to misleading dashboards and incorrect decisions.\n",
        "   \n",
        "    -->Poor data quality results in:\n",
        "\t•\tDuplicate records → inflated metrics (sales, revenue)\n",
        "\t•\tMissing values → incomplete KPIs\n",
        "\t•\tIncorrect values → wrong trends and insights\n",
        "\t•\tInconsistent dimensions → wrong aggregations\n",
        "\n",
        "As dashboards rely on processed data, any quality issue directly converts into misleading visuals, causing management to take incorrect business decisions.\n",
        "\n",
        "\n",
        "Question 3 : What is duplicate data? Explain three causes in ETL pipelines.\n",
        "\n",
        "    -->Duplicate data refers to multiple records representing the same real-world entity or transaction.\n",
        "\n",
        "Three causes in ETL pipelines:\n",
        "\t1.\tMultiple source systems sending the same data\n",
        "\t2.\tReprocessing files due to job failures\n",
        "\t3.\tMissing or incorrect business keys during data merge\n",
        "\n",
        "\n",
        "Question 4 : Differentiate between exact, partial, and fuzzy duplicates.\n",
        "\n",
        "    -->Exact Duplicate\n",
        "All fields are identical\n",
        "Same row repeated\n",
        "Partial Duplicate\n",
        "Some fields match\n",
        "Same customer, different city spelling\n",
        "Fuzzy Duplicate\n",
        "Similar but not exact\n",
        "“Rahul Mehta” vs “R. Mehta”\n",
        "\n",
        "\n",
        "Question 5 : Why should data validation be performed during transformation rather than after loading?\n",
        "  \n",
        "    -->Validation during transformation:\n",
        "\t•\tPrevents bad data from entering target systems\n",
        "\t•\tReduces reprocessing cost\n",
        "\t•\tImproves data trust\n",
        "\t•\tEnables early error detection\n",
        "\n",
        "Validating after loading risks polluting reporting tables and downstream analytics.\n",
        "\n",
        "\n",
        "Question 6 : Explain how business rules help in validating data accuracy. Give an example\n",
        "\n",
        "    -->Business rules define acceptable conditions for data values based on domain logic.\n",
        "\n",
        "Example:\n",
        "\t•\tQuantity must be > 0\n",
        "\t•\tTxn_Amount cannot be NULL\n",
        "\t•\tTxn_Date cannot be in the future\n",
        "\n",
        "In the dataset:\n",
        "\t•\tQuantity is NULL for Txn_ID 205 → violates business rule\n",
        "\t•\tTxn_Amount is NULL for Txn_ID 206 → invalid transaction\n",
        "\n",
        "\n",
        "Question 7 : Write an SQL query on to list all duplicate keys and their counts using the\n",
        "business key (Customer_ID + Product_ID + Txn_Date + Txn_Amount )\n",
        "\n",
        "    -->SELECT\n",
        "    Customer_ID,\n",
        "    Product_ID,\n",
        "    Txn_Date,\n",
        "    Txn_Amount,\n",
        "    COUNT(*) AS duplicate_count\n",
        "FROM Sales_Transactions\n",
        "GROUP BY\n",
        "    Customer_ID,\n",
        "    Product_ID,\n",
        "    Txn_Date,\n",
        "    Txn_Amount\n",
        "HAVING COUNT(*) > 1;\n",
        "Explanation:\n",
        "This query identifies duplicate business keys such as:\n",
        "\t•\tC101 + P11 + 2025-12-01 + 4000 (appears multiple times)\n",
        "\n",
        "\n",
        "Question 8 : Enforcing Referential Integrity\n",
        "\n",
        "    -->Identify Sales_Transactions.Customer_ID values that violate referential integrity and write a query to detect them.\n",
        "\n",
        "Customers_Master contains:\n",
        "\t•\tC101\n",
        "\t•\tC102\n",
        "\t•\tC103\n",
        "\t•\tC104\n",
        "\n",
        "Violating Customer_IDs in Sales_Transactions:\n",
        "\t•\tC105\n",
        "\t•\tC106\n",
        "\n",
        "SQL Query to detect violations:\n",
        "SELECT DISTINCT st.Customer_ID\n",
        "FROM Sales_Transactions st\n",
        "LEFT JOIN Customers_Master cm\n",
        "ON st.Customer_ID = cm.CustomerID\n",
        "WHERE cm.CustomerID IS NULL;\n",
        "Any Customer_ID in Sales_Transactions not present in Customers_Master violates referential integrity."
      ],
      "metadata": {
        "id": "Ca8fSG4MsWWb"
      }
    }
  ]
}